% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{g-retriever,
  title   = {G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering},
  author  = {Xiaoxin He and Yijun Tian and Yifei Sun and N. Chawla and Thomas Laurent and Yann LeCun and Xavier Bresson and Bryan Hooi},
  journal = {ArXiv},
  year    = {2024},
  volume  = {abs/2402.07630},
  url     = {https://api.semanticscholar.org/CorpusID:267626823}
}
@article{rag,
  title   = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author  = {Patrick Lewis and Ethan Perez and Aleksandara Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Kuttler and Mike Lewis and Wen-tau Yih and Tim Rockt{\"a}schel and Sebastian Riedel and Douwe Kiela},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/2005.11401},
  url     = {https://api.semanticscholar.org/CorpusID:218869575}
}
@inproceedings{petroni-etal-2019-language,
  title     = {Language Models as Knowledge Bases?},
  author    = {Petroni, Fabio  and
               Rockt{\"a}schel, Tim  and
               Riedel, Sebastian  and
               Lewis, Patrick  and
               Bakhtin, Anton  and
               Wu, Yuxiang  and
               Miller, Alexander},
  editor    = {Inui, Kentaro  and
               Jiang, Jing  and
               Ng, Vincent  and
               Wan, Xiaojun},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-1250/},
  doi       = {10.18653/v1/D19-1250},
  pages     = {2463--2473},
  abstract  = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {\textquotedblleft}fill-in-the-blank{\textquotedblright} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at \url{https://github.com/facebookresearch/LAMA}.}
}
