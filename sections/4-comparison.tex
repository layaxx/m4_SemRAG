\section{Comparison to selected papers}

This section contextualizes G-Retriever within the field of retrieval-augmented generation (RAG) by comparing it to selected models and highlighting key similarities and differences.

\subsection{Retrieval-Augmented Generation for    Knowledge-Intensive NLP Tasks}

The original RAG paper by \citet{rag} introduced the retrieval-augmented generation framework, enabling pre-trained language models to retrieve external knowledge during text generation.

Like G-Retriever, RAG retrieves relevant information based on embedding similarity.
However, whereas G-Retriever utilizes text-based graphs as its knowledge source, RAG operates on chunked Wikipedia articles.
Another key distinction is that RAG fine-tunes its retriever jointly with the generative model, whereas G-Retriever's retrieval component remains frozen during training.
Both models primarily target question-answering tasks.

\subsection{REALM: Retrieval-Augmented Language Model Pre-Training}

REALM \cite{realm} is also one of the early retrieval-augmented language models.
It employs masked language modeling for pre-training on large text corpora, followed by fine-tuning for open-domain question answering.
Like RAG, it uses Wikipedia articles as its external knowledge source rather than structured graphs.

A key distinction from G-Retriever is that REALM fine-tunes its retriever model alongside the generative model, propagating gradients across all retrieved documents.
Additionally, it leverages salient span masking, a technique not utilized in G-Retriever.

\subsection{In-Context Retrieval-Augmented Language Models}

This work \cite{in-context} demonstrates that the core benefits of RAG can be achieved using a simple approach: retrieving documents and prepending them to the query before passing it to an off-the-shelf LLM—without requiring any additional training.

Most RAG-based papers, including G-Retriever, modify LLM architectures to integrate retrieval, with varying degrees of trainable components.
In contrast, this in-context approach allows for immediate deployment and eliminates the need for training resources, making it well-suited for scenarios that rely on API-based LLMs.

In contrast to G-Retriever, which performs retrieval only once per query, this approach retrieves documents iteratively, potentially up to once per generated token.
The authors also demonstrate that general-purpose retrievers, such as BM25, can outperform embedding-based retrievers like those used in G-Retriever, particularly when combined with additional reranking.


\subsection{Internet-Augmented Dialogue Generation}

\citet{komeili-etal-2022-internet} describe a dialogue-focused system that grounds its responses using real-time information retrieved from the internet.
To achieve this, the system generates a query based on its context and submits it to a search engine such as Bing.
The retrieved documents are then incorporated into the context for generating the model’s response.

By leveraging live internet data, this approach ensures access to the most up-to-date information, similar to how a human might search for facts before answering a question.

Beyond the general concept of retrieving external knowledge to enhance generation quality, this approach shares little in common with G-Retriever.
While the authors of G-Retriever \cite{g-retriever} briefly mention the possibility of a "Chat-with-your-graph" system, they do not explore this idea in detail.
As a result, G-Retriever is not primarily designed as a dialogue-oriented system.

\subsection{Retrieval-Augmented Retriever Multimodal Language Modeling}

RA-CM3 is a retrieval-augmented multi-modal model in which both the knowledge sources and generated outputs can be either text or images \cite{multimodal}.

Similar to other RAG approaches, RA-CM3 employs Maximum Inner Product Search (MIPS) to retrieve relevant documents from a candidate pool. However, it requires a specialized multi-modal encoder, such as CLIP, to process both textual and visual data.

The authors demonstrate that RA-CM3 improves generation quality while reducing training resource requirements. Additionally, they explore alternative use cases, such as controlled image generation and image editing.

While G-Retriever is evaluated on spatial reasoning tasks, it does not support images as input or output. Instead, it relies on textual graph representations of scenes as its knowledge source.

\subsection{Retrieval meets Long Context Large Language Models}

How do retrieval augmentation and long context windows for LLMs compare, and how does a combination of both perform?
These are the central questions investigated by \citet{Xu2023RetrievalML} in their 2024 study.

The authors evaluate their system on a question-answering dataset in which some queries require multi-hop reasoning, similar to the tasks used for G-Retriever.
Their findings suggest that retrieval augmentation can match or even surpass the performance of models with significantly larger context windows while also reducing inference time.

\citet{Xu2023RetrievalML} further highlight that the lost-in-the-middle problem \textemdash{} where relevant information is buried within a long context window \textemdash{} can be mitigated through retrieval augmentation.
This aligns with the findings of G-Retriever, which also demonstrates improved performance over baseline models by incorporating only the most relevant subgraphs rather than the entire graph.


\subsection{Benchmarking Large Language Models in Retrieval-Augmented Generation}

\citet{benchmarking} identify several key challenges faced by RAG models:

\paragraph{Noise robustness} The ability to generate accurate responses when some retrieved documents are irrelevant.
\paragraph{Negative rejection} The ability to refrain from answering when sufficient information is unavailable.
\paragraph{Information integration} The ability to aggregate knowledge from multiple sources.
\paragraph{Counterfactual robustness} The ability to detect and resolve conflicting information.

While G-Retriever is not among the systems that have been examined for this paper, these could still be relevant.
One exception might be information integration, since G-Retriever only uses a single (sub-) graph to condition on.



\subsection{G-Retriever}

How does G-Retriever compare to these models?

The main distinction of G-Retriever is its focus on text-based graphs as knowledge sources, along with a subgraph construction step that is absent or not needed in other approaches.

Additionally, G-Retriever employs prompt tuning, which is not common in most RAG models.
Unlike some retrieval-augmented systems, G-Retriever's retrieval component remains frozen during training.
The language model itself is also frozen, with the graph encoder and projection layer being the only trainable parts.

While the authors mention the possibility of using the model in a dialog system, in this paper they use G-Retriever for question answering, which is a very popular use case for RAG systems.
