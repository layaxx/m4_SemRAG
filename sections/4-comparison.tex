\section{Comparison to selected papers}

This section will contextualize G-Retriever in the field of RAG by comparing it to selected papers, highlighting similarities and key differences.

\subsection{Retrieval-Augmented Generation for    Knowledge-Intensive NLP Tasks}

The original RAG paper by \citet{rag} introduced the RAG framework, which allows pre-trained language models to access external knowledge sources during generation.
Like G-Retriever, RAG uses Similarity of embeddings to retrieve relevant information from external sources, but instead of text-based graphs, it uses chunked Wikipedia articles as knowledge source.
Unlike G-Retriever, where the retrieval part is frozen and does not change during training, RAG uses a retriever model that is fine-tuned jointly with the generative model.
Both papers focus on question answering scenarios.

\subsection{REALM: Retrieval-Augmented Language Model Pre-Training}

REALM \cite{realm} is also one of the very early papers of RAG.
The authors use masked language modelling to pre-train a language model on a large corpus of text and then fine-tune it for Open-Domain Question Answering.
Like RAG, REALM uses Wikipedia articles as external data source, not graphs.

A key difference to the G-Retriever model is that REALM uses a retriever model that is fine-tuned jointly with the generative model, gradients calculated for all documents.
The use of salient span masking also sets it apart from G-Retriever.

\subsection{In-Context Retrieval-Augmented Language Models}

The core contribution of this paper is the proof that most benefits of RAG can be had with a very simple setup, where documents are retrieved and prepended to the query, which can then be given to any off-the-shelf LLM, without the need to perform any training at all \cite{in-context}.
Other papers usually modify the LLM architecture in some way to to retrieval augmentation, with various parts being frozen or trainable, depending on the specific paper.

Using unmodified LLMs has the advantage of not requiring training resources and enabling faster deployment, for example by using off-site LLMs via API-Access.



\subsection{Internet-Augmented Dialogue Generation}

In \cite{komeili-etal-2022-internet}, the authors describe a dialog focused system that is grounded with information retrieved from the internet at generation time.
To do this, their system generates a query based on its context, which they supply to a search engine such as Bing.
The documents that are retrieved by this search engine are then added to the context to generate the actual model response.

The real-time usage of the internet enables this approach to access the latest information available on the internet, similar to how a human would probably search for information.

Apart from the general concept of retrieving information and using it to improve the generation results, this approach has little in common with G-Retriever. While the authors of G-retriever, \citet{g-retriever}, mention in passing the option for a "Chat-with-your-graph" system, they do not explore this in their paper, and the resulting G-Retriever is not really a dialog focused system.

\subsection{Retrieval-Augmented Retriever Multimodal Language Modeling}

\subsection{Retrieval meets Long Context Large Language Models}

