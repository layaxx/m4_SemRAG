\section{Comparison to selected papers}

This section will contextualize G-Retriever in the field of RAG by comparing it to selected papers, highlighting similarities and key differences.

\subsection{Retrieval-Augmented Generation for    Knowledge-Intensive NLP Tasks}

The original RAG paper by \citet{rag} introduced the RAG framework, which allows pre-trained language models to access external knowledge sources during generation.
Like G-Retriever, RAG uses Similarity of embeddings to retrieve relevant information from external sources, but instead of text-based graphs, it uses chunked Wikipedia articles as knowledge source.
Unlike G-Retriever, where the retrieval part is frozen and does not change during training, RAG uses a retriever model that is fine-tuned jointly with the generative model.
Both papers focus on question answering scenarios.

\subsection{REALM: Retrieval-Augmented Language Model Pre-Training}

\subsection{In-Context Retrieval-Augmented Language Models}

\subsection{Internet-Augmented Dialogue Generation}

\subsection{Retrieval-Augmented Retriever Multimodal Language Modeling}

\subsection{Retrieval meets Long Context Large Language Models}

\subsection{Benchmarking Large Language Models in Retrieval-Augmented Generation}