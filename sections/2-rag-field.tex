\section{Retrieval Augmented Generation}
This section provides an overview of the research field of Retrieval-Augmented Generation (RAG), focusing on its origins and key contributions leading to the G-Retriever model.

RAG, a subfield of Natural Language Processing (NLP), has gained significant traction since its introduction in 2020. It combines retrieval-based and generation-based models to enhance text generation quality by incorporating information from retrieved documents.

The foundational work in this field was introduced by \cite{rag}, who proposed the RAG framework.
Their key innovation was enabling language models to access external knowledge sources during generation, allowing them to condition outputs on retrieved information in addition to the static knowledge embedded in their weights.
The authors demonstrated that external knowledge sources could be updated, expanded, and refined independently of the model, facilitating access to more up-to-date and diverse information without the computational cost of retraining large language models.

The core mechanism is straightforward: a retrieval component selects relevant documents based on the query, and this retrieved information is integrated into the generation process.
This can be done by simply prepending the retrieved text to the query or through more sophisticated mechanisms that enable end-to-end training \cite{in-context}.

Given its effectiveness in knowledge-intensive tasks, RAG is widely used in question answering.
The choice of knowledge source depends on the application, with common options including Wikipedia and scientific literature.
Some approaches leverage the internet for real-time information, benefiting from search engines like Google or Bing.
However, this also introduces potential noise and misinformation compared to curated sources.
If dedicated search engines are not used, document relevance can be determined using vector similarity methods.

A crucial design choice in RAG models is the number of retrieved documents, typically ranging from three to five per query.
Some models perform a single retrieval step per query, conditioning the entire response on a fixed set of documents, while others retrieve documents iterativelyâ€”up to once per generated token \cite{rag}.
Models also vary in what parts of the system are static or trainable, with some freezing the retrieval component and others fine-tuning it jointly with the generative model \cite{realm}.
Some systems allow for off-the-shelf language models to be used without any training, thus enabling the use of pre-trained models via API access.

Adding retrieval results to the generation process has been shown to improve model performance, often matching or surpassing significantly larger models that lack external knowledge access \cite{in-context}.
Moreover, retrieval enhances response quality, mitigates the limitations of small context windows, and reduces computational resource requirements \cite{Xu2023RetrievalML}.

% common evaluation methods/metrics?