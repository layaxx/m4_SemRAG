\section{Retrieval Augmented Generation}

This section will give an overview of the research field of Retrieval Augmented Generation (RAG), with a focus on its inception and key contributions leading up to the G-Retriever paper.

RAG is a subfield of Natural Language Processing that has gained traction since its inception in 2020. It combines the strengths of retrieval-based and generation-based models, aiming to improve the quality of generated text by incorporating information from retrieved documents.

The foundational work in this field was done by \cite{rag}, who introduced the RAG framework.
The main idea behind this paper was to allow pre-trained language models to access external knowledge sources during generation, thereby allowing the model to condition on information retrieved from external sources, in addition to  the knowledge embedded into its weight during the training process.
The authors show that such external knowledge sources can be updated, expanded, and refined independently of the model, which allows for more up-to-date and diverse information without the need to expend the resources associated with training large language models.