While Large Language Models (LLMs) have demonstrated strong performance across various Natural Language Processing (NLP) tasks, including question answering, they are prone to hallucinations and rely on static, potentially outdated information \cite{Marcus2020TheND}.
LLMs encode knowledge within their model weights \cite{petroni-etal-2019-language}, but their capacity is constrained by model size, and their knowledge remains fixed after training. Updating or modifying this knowledge is computationally expensive and requires full or partial retraining \cite{realm}.

Retrieval-Augmented Generation (RAG), a subfield of NLP, addresses these limitations by integrating retrieval-based and generation-based approaches.
By retrieving relevant information from external knowledge sources and incorporating it into the generation process, RAG enhances output accuracy and adaptability.